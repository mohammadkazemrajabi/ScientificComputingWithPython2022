# -*- coding: utf-8 -*-
"""08ex_algebra.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gEKiSLxXL7pNwHyLX9gbwJOYV5ZB_C8m
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy import linalg as la
import seaborn as sns
import pandas as pd

x1 = np.random.normal(0, 1, 1000)
x2=x1+np.random.normal(0, 3, 1000)
x3=2*x1+x2
print(x1.shape,x2.shape,x3.shape)

df=np.array([x1,x2,x3])
print(df.shape)
df

np_cov = np.cov(df)
print(np_cov.shape)
print(np_cov)

sns.scatterplot(x1, x2
               );

sns.scatterplot(x1, x3
               );

sns.scatterplot(x2, x3
               );

U, S, Vt = np.linalg.svd(df)
scale_factor = 3
# Rescale the SVD spectrum to get the eigenvalues
l_svd = S**2/(1000-1)
# The matrix U already contains the eigenvectors
V_svd = U
print(V_svd)
print(l_svd)



"""**Find the eigenvectors and eigenvalues using the eigendecomposition of the covariance matrix**"""

l, v = la.eig(np_cov)
print('eigenvalues')
print(l)
print('realeigenvalues')
print(np.real_if_close(l))
print('eigenvectors')
print(v)

"""**Find the eigenvectors and eigenvalues using the SVD. Check that the two procedures yield to same result**

compare eginvectors in both approaches and they are the same;
"""

U, s, Vt = la.svd(np_cov)
print('u:',U)
print(np.real_if_close(s))
print('\n')
print('s:',s)
print('\n')
print(v)
print(v.T.shape)

print(l)
print(s)
Lambda = np.diag(l)
print(Lambda)
print(Lambda.trace())
print(np_cov.trace())

print(Lambda[0][0]/np_cov.trace())#first axes for pca
print(Lambda[0][0]/Lambda.trace())#first axes for pca

print(np.real_if_close(Lambda[0][0]))
print(np.real_if_close(Lambda[1][1]+Lambda[0][0]))
print(Lambda[0][0])
print(Lambda[0][0]+Lambda[1][1]/np_cov.trace())#first axes for pca
print('first and second axes',np.real_if_close(Lambda[0][0]+Lambda[1][1]))
print('trace',np_cov.trace())
print(25.740803915776/27.753012580688488)
print(np.real_if_close(Lambda[1][1]+Lambda[0][0]+Lambda[2][2]))

#for each eigenvalue calculate:
print(la[0][0]/la.trace())
print(la[1][1]/la.trace())
print(la[2][2]/la.trace())
np.real_if_close(la[2][2])
print(la.trace())
print(4.38403696e-15/27.753012580688498
)

la=np.diag(s)
print(la)
print(la.trace())
print(la[0][0]/la.trace())
print(la[0][0]+la[1][1])
print(la[0][0]+la[1][1]/la.trace())
print(25.813308068579666/27.753012580688498)
print(la[0][0]+la[1][1]+la[2][2],la.trace())
print(27.753012580688498/np_cov.trace())

print(v[0][0])

ax = plt.axes(projection='3d')
plt.rcParams['figure.figsize'] = (10,10)
scale_factor = 3
ax.scatter(x1, x2, x3, linewidth=0.5,s=100);
for li, vi in zip(l,v.T):
    print("Eigenvalue:", li, ",\teigenvector:", vi)
    print("sld",vi[0])
    # the line is defined by specifying its beginning and its end 
    plt.plot([0, scale_factor * li * vi[0]], [0, scale_factor * li * vi[1]],[0, scale_factor * li * vi[2]], 'r-', lw=2)

ax = plt.axes(projection='3d')
plt.rcParams['figure.figsize'] = (10,10)
scale_factor = 3
ax.scatter(x1, x2, x3, linewidth=0.5,s=100);
for li, vi in zip(l_svd,V_svd.T ):
    print("Eigenvalue:", li, ",\teigenvector:", vi)
    print("sld",vi[0])
    # the line is defined by specifying its beginning and its end 
    plt.plot([0, scale_factor * li * vi[0]], [0, scale_factor * li * vi[1]],[0, scale_factor * li * vi[2]], 'r-', lw=2)

#select eigenvector according to largest eigen values
for li, vi in zip(l_svd,V_svd.T ):
    print("Eigenvalue:", li, ",\teigenvector:", vi)
print(V_svd.T)
print(l_svd[2],l_svd[1])
print(l_svd[2]>l_svd[1])
#the bigest eigen values are 29.177960131764372 ,29.177960131764372 
#so the eigenvector are
print(V_svd.T[0,:],V_svd[1,:])
newd=np.array([V_svd.T[0,:],V_svd.T[1,:]])
print('------')
print(newd)
print('now compare')
print(v.T)

# rotate all the data points accordingly to the new base by multiplying by the transpose of matrix V
Xp = np.dot(v.T, df)
scale_factor=6
ax = plt.axes(projection='3d')
plt.rcParams['figure.figsize'] = (10,10)
ax.scatter(Xp[0,:], Xp[1,:],Xp[2,:], alpha=0.1,s=100)
# same eigenvalues as before, assume we rotated properly the data
for li, vi in zip(l_svd,V_svd.T):
    print("Eigenvalue:", li, ",\teigenvector:", vi)
    print("sld",vi[0])
    # the line is defined by specifying its beginning and its end 
    plt.plot([0, scale_factor * li * vi[0]], [0, scale_factor * li * vi[1]],[0, scale_factor * li * vi[2]], 'r-', lw=6)

#now we have new basis so we plot our data on this eigenvectors
new_ds=np.dot(v.T, df)
fig, axes = plt.subplots(2, 3, figsize=(15,15))
axes[0,0].scatter(df[0],df[1] ,alpha=0.4,marker=r'$\clubsuit$')
axes[0,1].scatter(df[0],df[2] ,alpha=0.4,marker=r'$\clubsuit$')
axes[0,2].scatter(df[1],df[2] ,alpha=0.4,marker=r'$\clubsuit$')
axes[1,0].scatter(new_ds[0],new_ds[1] ,color="lightpink",alpha=0.6,marker=r'$\clubsuit$')
axes[1,1].scatter(new_ds[0],new_ds[2] ,color="lightpink",alpha=0.6,marker=r'$\clubsuit$')
axes[1,2].scatter(new_ds[1],new_ds[2] ,color="lightpink",alpha=0.6,marker=r'$\clubsuit$')

#ex2
noises = np.random.normal(loc=0, scale=1/50, size=(3, 1000, 10))
x_noise = df + np.sum(noises, axis = 2)
print(x_noise.shape)

U2, S2, Vt2 = np.linalg.svd(x_noise)
scale_factor = 3
# Rescale the SVD spectrum to get the eigenvalues
l_svd2 = S2**2/(1000-1)
# The matrix U already contains the eigenvectors
V_svd2 = U2
print(V_svd2)
print(l_svd2)

l2, v2 = la.eig(np.cov(x_noise))
print('eigenvalues')
print(l2)
print('realeigenvalues')
print(np.real_if_close(l))
print('eigenvectors')
print(v2)

ax = plt.axes(projection='3d')
plt.rcParams['figure.figsize'] = (10,10)
scale_factor = 3
ax.scatter(x_noise[0,:], x_noise[1,:], x_noise[2,:], linewidth=0.5,s=100);
for li, vi in zip(l2,v2.T):
    print("Eigenvalue:", li, ",\teigenvector:", vi)
    print("sld",vi[0])
    # the line is defined by specifying its beginning and its end 
    plt.plot([0, scale_factor * li * vi[0]], [0, scale_factor * li * vi[1]],[0, scale_factor * li * vi[2]], 'r-', lw=2)

Xp = np.dot(v.T, x_noise)
scale_factor=6
ax = plt.axes(projection='3d')
plt.rcParams['figure.figsize'] = (10,10)
ax.scatter(x_noise[0,:], x_noise[1,:],x_noise[2,:], alpha=0.1,s=100)
# same eigenvalues as before, assume we rotated properly the data
for li, vi in zip(l_svd2,V_svd2.T):
    print("Eigenvalue:", li, ",\teigenvector:", vi)
    print("sld",vi[0])
    # the line is defined by specifying its beginning and its end 
    plt.plot([0, scale_factor * li * vi[0]], [0, scale_factor * li * vi[1]],[0, scale_factor * li * vi[2]], 'r-', lw=6)

new_noise=np.dot(v2.T, df)
fig, axes = plt.subplots(2, 3, figsize=(15,15))
axes[0,0].scatter(x_noise[0],x_noise[1] ,alpha=0.4,marker=r'$\clubsuit$')
axes[0,1].scatter(x_noise[0],x_noise[2] ,alpha=0.4,marker=r'$\clubsuit$')
axes[0,2].scatter(x_noise[1],x_noise[2] ,alpha=0.4,marker=r'$\clubsuit$')
axes[1,0].scatter(new_noise[0],new_noise[1] ,color="lightpink",alpha=0.6,marker=r'$\clubsuit$')
axes[1,1].scatter(new_noise[0],new_noise[2] ,color="lightpink",alpha=0.6,marker=r'$\clubsuit$')
axes[1,2].scatter(new_noise[1],new_noise[2] ,color="lightpink",alpha=0.6,marker=r'$\clubsuit$')

# get the dataset and its description on the proper data directory
!wget https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data -P data/
!wget https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.names -P data/

#ex3
import pandas as pd

dataset = pd.read_csv("data/magic04.data",header = None)

dataset=dataset.iloc[:,:10] ## to compute the covariance i need to get rid of the last column which has not numeric number values
covariance = dataset.cov()

l, v = la.eig(covariance)
l = np.real_if_close(l)
xp = np.dot(v.T,dataset.T)

fig, axs = plt.subplots(2,3,figsize=(32,15))
axs[0][0].set_title("x0 vs x1")
axs[0][0].scatter(dataset.iloc[:,0],dataset.iloc[:,1])
axs[0][1].set_title("x0 vs x2")
axs[0][1].scatter(dataset.iloc[:,0],dataset.iloc[:,2])
axs[0][2].set_title("x1 vs x2")
axs[0][2].scatter(dataset.iloc[:,1],dataset.iloc[:,2])

axs[1][0].set_title("xp0 vs xp1")
axs[1][0].scatter(xp[0,:],xp[1,:])
axs[1][1].set_title("xp0 vs xp2")
axs[1][1].scatter(xp[0,:],xp[2,:])
axs[1][2].set_title("xp1 vs xp2")
axs[1][2].scatter(xp[1,:],xp[2,:])

plt.show()

"""ex3 optional

"""

# get the dataset and its description on the proper data directory
!wget https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data -P data/
!wget https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.names -P data/

file_name="/content/data/magic04.data"
head="/content/data/magic04.names"
df=pd.read_csv(file_name, names=["fLength","fWidth","fSize","fConc","fConc1","fAsym","fM3Long","fM3Trans","fAlpha","fDist"])
cov_m=np.cov(df.iloc[:,:-2].T)
l, V = la.eig(cov_m)
#PCA 
dataPCA=np.dot(V.T, df.iloc[:,:-2].T)

dataPCA